{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V1Ir7_g0yvWR",
        "outputId": "3d6d48ce-bfbd-4939-b62e-cfe9f4478899"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating sample supply chain data...\n",
            "Raw dataset saved with 1000 records.\n",
            "Preprocessing data...\n",
            "Detecting supply chain anomalies...\n",
            "Detected 50 anomalies out of 1000 records.\n",
            "Analyzing root causes...\n",
            "\n",
            "Example Root Causes for Detected Anomalies:\n",
            "\n",
            "Anomaly #1 (Record #3):\n",
            "  Date: 2024-07-06 00:00:00, Region: North, Supplier: Supplier E\n",
            "  Potential Root Causes:\n",
            "    - Demand Forecast is abnormally high (z-score: 6.55)\n",
            "    - Capacity Utilization is abnormally high (z-score: 5.04)\n",
            "    - Inventory Level is abnormally low (z-score: -3.27)\n",
            "\n",
            "Anomaly #2 (Record #13):\n",
            "  Date: 2025-03-16 00:00:00, Region: East, Supplier: Supplier A\n",
            "  Potential Root Causes:\n",
            "    - Demand Forecast is abnormally high (z-score: 6.52)\n",
            "    - Capacity Utilization is abnormally high (z-score: 4.45)\n",
            "    - Inventory Level is abnormally low (z-score: -2.76)\n",
            "\n",
            "Anomaly #3 (Record #24):\n",
            "  Date: 2025-01-29 00:00:00, Region: South, Supplier: Supplier C\n",
            "  Potential Root Causes:\n",
            "    - Capacity Utilization is abnormally high (z-score: 6.42)\n",
            "    - Demand Forecast is abnormally high (z-score: 6.30)\n",
            "    - Inventory Level is abnormally low (z-score: -3.02)\n",
            "\n",
            "Anomaly #4 (Record #36):\n",
            "  Date: 2024-05-11 00:00:00, Region: Central, Supplier: Supplier C\n",
            "  Potential Root Causes:\n",
            "    - Transport Time Days is abnormally high (z-score: 6.97)\n",
            "    - Capacity Utilization is abnormally high (z-score: 6.15)\n",
            "    - Production Capacity is abnormally low (z-score: -5.73)\n",
            "\n",
            "Anomaly #5 (Record #39):\n",
            "  Date: 2024-11-20 00:00:00, Region: West, Supplier: Supplier E\n",
            "  Potential Root Causes:\n",
            "    - Transport Time Days is abnormally high (z-score: 52.03)\n",
            "    - Lead Time Days is abnormally high (z-score: 12.72)\n",
            "    - Inventory Coverage Days is abnormally high (z-score: 5.72)\n",
            "\n",
            "Creating visualizations...\n",
            "\n",
            "Analysis complete! Files saved:\n",
            "- supply_chain_data.csv: Original dataset\n",
            "- supply_chain_with_anomalies.csv: Dataset with anomaly flags\n",
            "- anomaly_detection_pca.png: PCA visualization of anomalies\n",
            "- anomaly_time_series.png: Time series plots with highlighted anomalies\n",
            "- feature_correlation.png: Correlation matrix of supply chain features\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 1. Generate Sample Dataset\n",
        "def generate_sample_data(n_samples=1000):\n",
        "    \"\"\"\n",
        "    Generate synthetic supply chain data for FMCG company\n",
        "    \"\"\"\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Date range for the last year\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=365)\n",
        "    dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days)]\n",
        "\n",
        "    # Randomly select dates (with replacement to simulate multiple events per day)\n",
        "    selected_dates = np.random.choice(dates, n_samples)\n",
        "    selected_dates = [date.strftime('%Y-%m-%d') for date in selected_dates]\n",
        "\n",
        "    # Product categories\n",
        "    categories = ['Beverages', 'Personal Care', 'Home Care', 'Food', 'Snacks']\n",
        "    selected_categories = np.random.choice(categories, n_samples)\n",
        "\n",
        "    # Regions\n",
        "    regions = ['North', 'South', 'East', 'West', 'Central']\n",
        "    selected_regions = np.random.choice(regions, n_samples)\n",
        "\n",
        "    # Suppliers\n",
        "    suppliers = ['Supplier A', 'Supplier B', 'Supplier C', 'Supplier D', 'Supplier E']\n",
        "    selected_suppliers = np.random.choice(suppliers, n_samples)\n",
        "\n",
        "    # Normal operational metrics\n",
        "    order_quantities = np.random.normal(5000, 1000, n_samples)\n",
        "    lead_times = np.random.normal(5, 1, n_samples)\n",
        "    transport_times = np.random.normal(3, 0.5, n_samples)\n",
        "    inventory_levels = np.random.normal(8000, 1500, n_samples)\n",
        "    demand_forecast = np.random.normal(4800, 900, n_samples)\n",
        "    production_capacity = np.random.normal(6000, 500, n_samples)\n",
        "\n",
        "    # Introduce seasonal patterns\n",
        "    seasonal_factor = np.sin(np.linspace(0, 2*np.pi, 365))\n",
        "    seasonal_indices = [dates.index(datetime.strptime(date, '%Y-%m-%d').date())\n",
        "                        if datetime.strptime(date, '%Y-%m-%d').date() in dates\n",
        "                        else 0 for date in selected_dates]\n",
        "\n",
        "    seasonal_effect = [seasonal_factor[i % len(seasonal_factor)] for i in seasonal_indices]\n",
        "\n",
        "    # Ensure seasonal_effect has the same length as demand_forecast\n",
        "    seasonal_effect = np.array(seasonal_effect)[:n_samples]\n",
        "\n",
        "    demand_forecast = demand_forecast + seasonal_effect * 500\n",
        "\n",
        "    # Introduce some anomalies (about 5% of the data)\n",
        "    anomaly_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
        "\n",
        "    # Supply disruption anomalies\n",
        "    for idx in anomaly_indices[:len(anomaly_indices)//3]:\n",
        "        lead_times[idx] *= np.random.uniform(2, 3)  # Significantly longer lead times\n",
        "        inventory_levels[idx] *= np.random.uniform(0.2, 0.5)  # Much lower inventory\n",
        "\n",
        "    # Demand spike anomalies\n",
        "    for idx in anomaly_indices[len(anomaly_indices)//3:2*len(anomaly_indices)//3]:\n",
        "        demand_forecast[idx] *= np.random.uniform(1.5, 2.5)  # Unexpected demand spikes\n",
        "        inventory_levels[idx] *= np.random.uniform(0.3, 0.6)  # Lower inventory due to spikes\n",
        "\n",
        "    # Production issues anomalies\n",
        "    for idx in anomaly_indices[2*len(anomaly_indices)//3:]:\n",
        "        production_capacity[idx] *= np.random.uniform(0.4, 0.7)  # Production capacity issues\n",
        "        transport_times[idx] *= np.random.uniform(1.5, 2.5)  # Logistics delays\n",
        "\n",
        "    # Weather impact (random severe weather days)\n",
        "    weather_impact = np.zeros(n_samples)\n",
        "    severe_weather_days = np.random.choice(n_samples, size=int(n_samples * 0.03), replace=False)\n",
        "    weather_impact[severe_weather_days] = np.random.uniform(0.5, 1.0, size=len(severe_weather_days))\n",
        "\n",
        "    # Create DataFrame\n",
        "    df = pd.DataFrame({\n",
        "        'date': selected_dates,\n",
        "        'category': selected_categories,\n",
        "        'region': selected_regions,\n",
        "        'supplier': selected_suppliers,\n",
        "        'order_quantity': order_quantities,\n",
        "        'lead_time_days': lead_times,\n",
        "        'transport_time_days': transport_times,\n",
        "        'inventory_level': inventory_levels,\n",
        "        'demand_forecast': demand_forecast,\n",
        "        'production_capacity': production_capacity,\n",
        "        'weather_impact': weather_impact,\n",
        "        # Derived metrics\n",
        "        'inventory_coverage_days': inventory_levels / (demand_forecast / 30),\n",
        "        'capacity_utilization': demand_forecast / production_capacity,\n",
        "    })\n",
        "\n",
        "    # Add a few extreme outliers\n",
        "    extreme_indices = np.random.choice(n_samples, size=10, replace=False)\n",
        "    df.loc[extreme_indices, 'lead_time_days'] *= 5\n",
        "    df.loc[extreme_indices, 'transport_time_days'] *= 4\n",
        "\n",
        "    # Label the true anomalies for evaluation\n",
        "    df['true_anomaly'] = 0\n",
        "    df.loc[anomaly_indices, 'true_anomaly'] = 1\n",
        "    df.loc[extreme_indices, 'true_anomaly'] = 1\n",
        "    df.loc[severe_weather_days, 'true_anomaly'] = 1\n",
        "\n",
        "    return df\n",
        "\n",
        "# 2. Data Preprocessing\n",
        "def preprocess_data(df):\n",
        "    \"\"\"\n",
        "    Preprocess the supply chain data for anomaly detection\n",
        "    \"\"\"\n",
        "    # Convert date to datetime\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "    # One-hot encode categorical variables\n",
        "    df_processed = pd.get_dummies(df, columns=['category', 'region', 'supplier'])\n",
        "\n",
        "    # Extract features for anomaly detection\n",
        "    features = ['order_quantity', 'lead_time_days', 'transport_time_days',\n",
        "                'inventory_level', 'demand_forecast', 'production_capacity',\n",
        "                'weather_impact', 'inventory_coverage_days', 'capacity_utilization']\n",
        "\n",
        "    # Scale the features\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = pd.DataFrame(\n",
        "        scaler.fit_transform(df_processed[features]),\n",
        "        columns=features\n",
        "    )\n",
        "\n",
        "    return df_scaled, features, scaler, df_processed\n",
        "\n",
        "# 3. Implement Multiple Anomaly Detection Methods\n",
        "\n",
        "# a. Isolation Forest\n",
        "def isolation_forest_detection(X, contamination=0.05):\n",
        "    \"\"\"\n",
        "    Detect anomalies using Isolation Forest\n",
        "    \"\"\"\n",
        "    model = IsolationForest(\n",
        "        n_estimators=100,\n",
        "        max_samples='auto',\n",
        "        contamination=contamination,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Fit and predict\n",
        "    y_pred = model.fit_predict(X)\n",
        "\n",
        "    # Convert predictions to binary (1 for normal, -1 for anomaly)\n",
        "    # Convert to 0 for normal, 1 for anomaly to make it easier to understand\n",
        "    anomalies = np.where(y_pred == -1, 1, 0)\n",
        "\n",
        "    return anomalies, model\n",
        "\n",
        "# b. DBSCAN Clustering\n",
        "def dbscan_detection(X, eps=0.5, min_samples=5):\n",
        "    \"\"\"\n",
        "    Detect anomalies using DBSCAN clustering\n",
        "    \"\"\"\n",
        "    model = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "    clusters = model.fit_predict(X)\n",
        "\n",
        "    # Points labeled as -1 are considered outliers in DBSCAN\n",
        "    anomalies = np.where(clusters == -1, 1, 0)\n",
        "\n",
        "    return anomalies, model\n",
        "\n",
        "# 4. Ensemble Method\n",
        "def ensemble_anomaly_detection(X):\n",
        "    \"\"\"\n",
        "    Combine multiple anomaly detection methods\n",
        "    \"\"\"\n",
        "    # Apply different methods\n",
        "    isolation_anomalies, iso_model = isolation_forest_detection(X)\n",
        "\n",
        "    # Apply PCA for dimensionality reduction before DBSCAN\n",
        "    pca = PCA(n_components=0.95)  # Retain 95% of variance\n",
        "    X_pca = pca.fit_transform(X)\n",
        "\n",
        "    # Determine eps parameter adaptively\n",
        "    from sklearn.neighbors import NearestNeighbors\n",
        "    nn = NearestNeighbors(n_neighbors=2)\n",
        "    nn.fit(X_pca)\n",
        "    distances, _ = nn.kneighbors(X_pca)\n",
        "    distances = np.sort(distances[:, 1])\n",
        "    knee_point = np.argmax(distances[1:] - distances[:-1]) + 1\n",
        "    eps = distances[knee_point]\n",
        "\n",
        "    dbscan_anomalies, db_model = dbscan_detection(X_pca, eps=eps, min_samples=5)\n",
        "\n",
        "    # Combine methods (a point is anomalous if either method flags it)\n",
        "    ensemble_anomalies = np.logical_or(isolation_anomalies, dbscan_anomalies).astype(int)\n",
        "\n",
        "    return {\n",
        "        'isolation_forest': isolation_anomalies,\n",
        "        'dbscan': dbscan_anomalies,\n",
        "        'ensemble': ensemble_anomalies\n",
        "    }, iso_model, db_model, pca\n",
        "\n",
        "# 5. Root Cause Analysis\n",
        "def identify_root_causes(df, anomaly_indices, features, scaler):\n",
        "    \"\"\"\n",
        "    Identify potential root causes for detected anomalies\n",
        "    \"\"\"\n",
        "    anomalous_data = df.iloc[anomaly_indices]\n",
        "    normal_data = df.iloc[~np.isin(np.arange(len(df)), anomaly_indices)]\n",
        "\n",
        "    # Calculate z-scores for each feature in anomalous data\n",
        "    means = normal_data[features].mean()\n",
        "    stds = normal_data[features].std()\n",
        "\n",
        "    z_scores = (anomalous_data[features] - means) / stds\n",
        "\n",
        "    # Identify significant deviations\n",
        "    significant_features = {}\n",
        "    for idx, row in z_scores.iterrows():\n",
        "        significant_dev = []\n",
        "        for feature, value in row.items():\n",
        "            if abs(value) > 2:  # Z-score threshold\n",
        "                direction = \"high\" if value > 0 else \"low\"\n",
        "                significant_dev.append((feature, value, direction))\n",
        "\n",
        "        # Sort by absolute z-score to find most significant deviations\n",
        "        significant_dev.sort(key=lambda x: abs(x[1]), reverse=True)\n",
        "        significant_features[idx] = significant_dev\n",
        "\n",
        "    return significant_features\n",
        "\n",
        "# 6. Visualization and Reporting\n",
        "def visualize_anomalies(df, original_df, anomaly_results, features, pca):\n",
        "    \"\"\"\n",
        "    Create visualizations for detected anomalies\n",
        "    \"\"\"\n",
        "    # 1. PCA visualization of anomalies\n",
        "    X_pca = pca.transform(df)\n",
        "\n",
        "    plt.figure(figsize=(12, 10))\n",
        "\n",
        "    # Create a subplot for each detection method\n",
        "    methods = list(anomaly_results.keys())\n",
        "\n",
        "    for i, method in enumerate(methods):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "        plt.scatter(X_pca[:, 0], X_pca[:, 1], c=anomaly_results[method],\n",
        "                   cmap='viridis', alpha=0.7)\n",
        "        plt.title(f'Anomalies detected by {method.replace(\"_\", \" \").title()}')\n",
        "        plt.xlabel('Principal Component 1')\n",
        "        plt.ylabel('Principal Component 2')\n",
        "        plt.colorbar(label='Anomaly (1) / Normal (0)')\n",
        "\n",
        "    # Add a subplot for true anomalies\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=original_df['true_anomaly'],\n",
        "               cmap='viridis', alpha=0.7)\n",
        "    plt.title('True Anomalies (for validation)')\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.colorbar(label='Anomaly (1) / Normal (0)')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('anomaly_detection_pca.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 2. Time series plot with highlighted anomalies\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    for i, feature in enumerate(['lead_time_days', 'inventory_coverage_days',\n",
        "                                'capacity_utilization', 'transport_time_days']):\n",
        "        plt.subplot(2, 2, i+1)\n",
        "\n",
        "        # Sort by date for time series\n",
        "        sorted_indices = np.argsort(original_df['date'])\n",
        "        dates = original_df['date'].iloc[sorted_indices]\n",
        "        values = original_df[feature].iloc[sorted_indices]\n",
        "        anomalies = anomaly_results['ensemble'][sorted_indices]\n",
        "\n",
        "        plt.plot(dates, values, 'b-', alpha=0.6, label=feature)\n",
        "        plt.scatter(dates[anomalies == 1], values[anomalies == 1],\n",
        "                   color='red', label='Detected Anomalies')\n",
        "\n",
        "        plt.title(f'Time Series of {feature.replace(\"_\", \" \").title()} with Anomalies')\n",
        "        plt.xlabel('Date')\n",
        "        plt.ylabel(feature.replace(\"_\", \" \").title())\n",
        "        plt.xticks(rotation=45)\n",
        "        plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('anomaly_time_series.png')\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Feature correlation matrix\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    corr = original_df[features].corr()\n",
        "    sns.heatmap(corr, annot=True, cmap='coolwarm', vmin=-1, vmax=1)\n",
        "    plt.title('Feature Correlation Matrix')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('feature_correlation.png')\n",
        "    plt.close()\n",
        "\n",
        "    return\n",
        "\n",
        "# 7. Main Function\n",
        "def main():\n",
        "    # Generate sample data\n",
        "    print(\"Generating sample supply chain data...\")\n",
        "    df = generate_sample_data(n_samples=1000)\n",
        "\n",
        "    # Save the raw dataset\n",
        "    df.to_csv('supply_chain_data.csv', index=False)\n",
        "    print(f\"Raw dataset saved with {len(df)} records.\")\n",
        "\n",
        "    # Preprocess data\n",
        "    print(\"Preprocessing data...\")\n",
        "    X_scaled, features, scaler, df_processed = preprocess_data(df)\n",
        "\n",
        "    # Detect anomalies\n",
        "    print(\"Detecting supply chain anomalies...\")\n",
        "    anomaly_results, iso_model, db_model, pca = ensemble_anomaly_detection(X_scaled)\n",
        "\n",
        "    # Identify anomalies from ensemble method\n",
        "    anomaly_indices = np.where(anomaly_results['ensemble'] == 1)[0]\n",
        "    print(f\"Detected {len(anomaly_indices)} anomalies out of {len(df)} records.\")\n",
        "\n",
        "    # Perform root cause analysis\n",
        "    print(\"Analyzing root causes...\")\n",
        "    root_causes = identify_root_causes(df, anomaly_indices, features, scaler)\n",
        "\n",
        "    # Output some example root causes\n",
        "    print(\"\\nExample Root Causes for Detected Anomalies:\")\n",
        "    for i, (idx, causes) in enumerate(list(root_causes.items())[:5]):\n",
        "        print(f\"\\nAnomaly #{i+1} (Record #{idx}):\")\n",
        "        print(f\"  Date: {df.iloc[idx]['date']}, Region: {df.iloc[idx]['region']}, Supplier: {df.iloc[idx]['supplier']}\")\n",
        "        print(\"  Potential Root Causes:\")\n",
        "        for feature, z_score, direction in causes[:3]:\n",
        "            print(f\"    - {feature.replace('_', ' ').title()} is abnormally {direction} (z-score: {z_score:.2f})\")\n",
        "\n",
        "    # Visualize results\n",
        "    print(\"\\nCreating visualizations...\")\n",
        "    visualize_anomalies(X_scaled, df, anomaly_results, features, pca)\n",
        "\n",
        "    # Add anomaly flags to the original data\n",
        "    df['anomaly_detected'] = anomaly_results['ensemble']\n",
        "    df.to_csv('supply_chain_with_anomalies.csv', index=False)\n",
        "\n",
        "    print(\"\\nAnalysis complete! Files saved:\")\n",
        "    print(\"- supply_chain_data.csv: Original dataset\")\n",
        "    print(\"- supply_chain_with_anomalies.csv: Dataset with anomaly flags\")\n",
        "    print(\"- anomaly_detection_pca.png: PCA visualization of anomalies\")\n",
        "    print(\"- anomaly_time_series.png: Time series plots with highlighted anomalies\")\n",
        "    print(\"- feature_correlation.png: Correlation matrix of supply chain features\")\n",
        "\n",
        "    return df, anomaly_results, root_causes\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install dash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBR0DQ3JAvcS",
        "outputId": "f6547600-9f83-47d3-d6ef-ab099bd94e87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dash\n",
            "  Downloading dash-3.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting Flask<3.1,>=1.0.4 (from dash)\n",
            "  Downloading flask-3.0.3-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting Werkzeug<3.1 (from dash)\n",
            "  Downloading werkzeug-3.0.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from dash) (5.24.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from dash) (8.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from dash) (4.12.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from dash) (2.32.3)\n",
            "Collecting retrying (from dash)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.11/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from dash) (75.1.0)\n",
            "Collecting stringcase>=1.2.0 (from dash)\n",
            "  Downloading stringcase-1.2.0.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.6.2 in /usr/local/lib/python3.11/dist-packages (from Flask<3.1,>=1.0.4->dash) (1.9.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from plotly>=5.0.0->dash) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from Werkzeug<3.1->dash) (3.0.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->dash) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->dash) (2025.1.31)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from retrying->dash) (1.17.0)\n",
            "Downloading dash-3.0.0-py3-none-any.whl (8.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m50.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask-3.0.3-py3-none-any.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading werkzeug-3.0.6-py3-none-any.whl (227 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m228.0/228.0 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: stringcase\n",
            "  Building wheel for stringcase (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=df06c8db3ccedd6888494599bdca9798a6a7115afae2b850482b46b435237af3\n",
            "  Stored in directory: /root/.cache/pip/wheels/b4/33/6d/d0820be98063da218c3206fbad2381cd2db3fbb1a0f0d254b5\n",
            "Successfully built stringcase\n",
            "Installing collected packages: stringcase, Werkzeug, retrying, Flask, dash\n",
            "  Attempting uninstall: Werkzeug\n",
            "    Found existing installation: Werkzeug 3.1.3\n",
            "    Uninstalling Werkzeug-3.1.3:\n",
            "      Successfully uninstalled Werkzeug-3.1.3\n",
            "  Attempting uninstall: Flask\n",
            "    Found existing installation: Flask 3.1.0\n",
            "    Uninstalling Flask-3.1.0:\n",
            "      Successfully uninstalled Flask-3.1.0\n",
            "Successfully installed Flask-3.0.3 Werkzeug-3.0.6 dash-3.0.0 retrying-1.3.4 stringcase-1.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import dash\n",
        "from dash import dcc, html, Input, Output, dash_table\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Import our anomaly detection module\n",
        "# In a real project, you would import from the previous file\n",
        "# For hackathon purposes, let's assume we have the data and models ready\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Sample data loading function (replace with your actual data)\n",
        "def load_data():\n",
        "    try:\n",
        "        # Try to load the generated data from previous script\n",
        "        df = pd.read_csv('supply_chain_with_anomalies.csv')\n",
        "        df['date'] = pd.to_datetime(df['date'])\n",
        "        return df\n",
        "    except:\n",
        "        # If file doesn't exist, generate new data using the function from previous script\n",
        "        # This would import the generate_sample_data function from the previous file\n",
        "        # For demo purposes, we'll include a simplified version\n",
        "        np.random.seed(42)\n",
        "        n_samples = 1000\n",
        "\n",
        "        # Date range for the last year\n",
        "        end_date = datetime.now()\n",
        "        start_date = end_date - timedelta(days=365)\n",
        "        dates = [start_date + timedelta(days=x) for x in range((end_date - start_date).days)]\n",
        "        selected_dates = np.random.choice(dates, n_samples)\n",
        "        selected_dates = [date.strftime('%Y-%m-%d') for date in selected_dates]\n",
        "\n",
        "        categories = ['Beverages', 'Personal Care', 'Home Care', 'Food', 'Snacks']\n",
        "        regions = ['North', 'South', 'East', 'West', 'Central']\n",
        "        suppliers = ['Supplier A', 'Supplier B', 'Supplier C', 'Supplier D', 'Supplier E']\n",
        "\n",
        "        df = pd.DataFrame({\n",
        "            'date': pd.to_datetime(selected_dates),\n",
        "            'category': np.random.choice(categories, n_samples),\n",
        "            'region': np.random.choice(regions, n_samples),\n",
        "            'supplier': np.random.choice(suppliers, n_samples),\n",
        "            'order_quantity': np.random.normal(5000, 1000, n_samples),\n",
        "            'lead_time_days': np.random.normal(5, 1, n_samples),\n",
        "            'transport_time_days': np.random.normal(3, 0.5, n_samples),\n",
        "            'inventory_level': np.random.normal(8000, 1500, n_samples),\n",
        "            'demand_forecast': np.random.normal(4800, 900, n_samples),\n",
        "            'production_capacity': np.random.normal(6000, 500, n_samples),\n",
        "            'weather_impact': np.random.uniform(0, 0.3, n_samples),\n",
        "        })\n",
        "\n",
        "        # Create some derived metrics\n",
        "        df['inventory_coverage_days'] = df['inventory_level'] / (df['demand_forecast'] / 30)\n",
        "        df['capacity_utilization'] = df['demand_forecast'] / df['production_capacity']\n",
        "\n",
        "        # Add random anomalies (5% of data)\n",
        "        anomaly_indices = np.random.choice(n_samples, size=int(n_samples * 0.05), replace=False)\n",
        "        df['anomaly_detected'] = 0\n",
        "        df.loc[anomaly_indices, 'anomaly_detected'] = 1\n",
        "\n",
        "        return df\n",
        "\n",
        "# Initialize the Dash app\n",
        "app = dash.Dash(__name__,\n",
        "                meta_tags=[{\"name\": \"viewport\", \"content\": \"width=device-width, initial-scale=1\"}],\n",
        "                title=\"Supply Chain Anomaly Detection\")\n",
        "\n",
        "# Load data\n",
        "df = load_data()\n",
        "\n",
        "# Define app layout\n",
        "app.layout = html.Div([\n",
        "    # Header\n",
        "    html.Div([\n",
        "        html.H1(\"FMCG Supply Chain Anomaly Detection\",\n",
        "                style={\"margin-bottom\": \"0px\", \"color\": \"white\"}),\n",
        "        html.H4(\"Real-time Monitoring & Root Cause Analysis Dashboard\",\n",
        "                style={\"margin-top\": \"0px\", \"color\": \"white\"})\n",
        "    ], style={\"text-align\": \"center\", \"padding\": \"1rem\", \"background-color\": \"#2c3e50\"}),\n",
        "\n",
        "    # Filters row\n",
        "    html.Div([\n",
        "        html.Div([\n",
        "            html.P(\"Date Range:\"),\n",
        "            dcc.DatePickerRange(\n",
        "                id='date-range',\n",
        "                min_date_allowed=df['date'].min().date(),\n",
        "                max_date_allowed=df['date'].max().date(),\n",
        "                start_date=df['date'].min().date(),\n",
        "                end_date=df['date'].max().date()\n",
        "            )\n",
        "        ], style={\"width\": \"25%\", \"display\": \"inline-block\", \"padding\": \"10px\"}),\n",
        "\n",
        "        html.Div([\n",
        "            html.P(\"Region:\"),\n",
        "            dcc.Dropdown(\n",
        "                id='region-filter',\n",
        "                options=[{\"label\": r, \"value\": r} for r in sorted(df['region'].unique())],\n",
        "                value=[],\n",
        "                multi=True,\n",
        "                placeholder=\"Select regions...\"\n",
        "            )\n",
        "        ], style={\"width\": \"20%\", \"display\": \"inline-block\", \"padding\": \"10px\"}),\n",
        "\n",
        "        html.Div([\n",
        "            html.P(\"Category:\"),\n",
        "            dcc.Dropdown(\n",
        "                id='category-filter',\n",
        "                options=[{\"label\": c, \"value\": c} for c in sorted(df['category'].unique())],\n",
        "                value=[],\n",
        "                multi=True,\n",
        "                placeholder=\"Select categories...\"\n",
        "            )\n",
        "        ], style={\"width\": \"20%\", \"display\": \"inline-block\", \"padding\": \"10px\"}),\n",
        "\n",
        "        html.Div([\n",
        "            html.P(\"Supplier:\"),\n",
        "            dcc.Dropdown(\n",
        "                id='supplier-filter',\n",
        "                options=[{\"label\": s, \"value\": s} for s in sorted(df['supplier'].unique())],\n",
        "                value=[],\n",
        "                multi=True,\n",
        "                placeholder=\"Select suppliers...\"\n",
        "            )\n",
        "        ], style={\"width\": \"20%\", \"display\": \"inline-block\", \"padding\": \"10px\"}),\n",
        "\n",
        "        html.Div([\n",
        "            html.P(\"Show Anomalies Only:\"),\n",
        "            dcc.RadioItems(\n",
        "                id='anomaly-filter',\n",
        "                options=[\n",
        "                    {'label': 'All Data', 'value': 'all'},\n",
        "                    {'label': 'Anomalies Only', 'value': 'anomalies'}\n",
        "                ],\n",
        "                value='all',\n",
        "                labelStyle={'display': 'inline-block', 'margin-right': '10px'}\n",
        "            )\n",
        "        ], style={\"width\": \"15%\", \"display\": \"inline-block\", \"padding\": \"10px\"})\n",
        "    ], style={\"background-color\": \"#f2f2f2\", \"padding\": \"10px\", \"margin\": \"10px 0px\"}),\n",
        "\n",
        "    # KPI Cards\n",
        "    html.Div([\n",
        "        html.Div([\n",
        "            html.H4(\"Total Records\", style={\"text-align\": \"center\"}),\n",
        "            html.P(id=\"total-records\", style={\"text-align\": \"center\", \"font-size\": \"24px\", \"font-weight\": \"bold\"})\n",
        "        ], className=\"kpi-card\"),\n",
        "\n",
        "        html.Div([\n",
        "            html.H4(\"Detected Anomalies\", style={\"text-align\": \"center\"}),\n",
        "            html.P(id=\"total-anomalies\", style={\"text-align\": \"center\", \"font-size\": \"24px\", \"font-weight\": \"bold\", \"color\": \"#e74c3c\"})\n",
        "        ], className=\"kpi-card\"),\n",
        "\n",
        "        html.Div([\n",
        "            html.H4(\"Anomaly Rate\", style={\"text-align\": \"center\"}),\n",
        "            html.P(id=\"anomaly-rate\", style={\"text-align\": \"center\", \"font-size\": \"24px\", \"font-weight\": \"bold\"})\n",
        "        ], className=\"kpi-card\"),\n",
        "\n",
        "        html.Div([\n",
        "            html.H4(\"Most Affected Region\", style={\"text-align\": \"center\"}),\n",
        "            html.P(id=\"most-affected-region\", style={\"text-align\": \"center\", \"font-size\": \"24px\", \"font-weight\": \"bold\"})\n",
        "        ], className=\"kpi-card\"),\n",
        "    ], style={\"display\": \"flex\", \"justify-content\": \"space-between\", \"margin\": \"20px 0px\"}),\n",
        "\n",
        "    # Main charts row\n",
        "    html.Div([\n",
        "        # Left column - Time series\n",
        "        html.Div([\n",
        "            html.H3(\"Supply Chain Metrics Over Time\", style={\"text-align\": \"center\"}),\n",
        "            dcc.Dropdown(\n",
        "                id='metric-selector',\n",
        "                options=[\n",
        "                    {'label': 'Lead Time (days)', 'value': 'lead_time_days'},\n",
        "                    {'label': 'Transport Time (days)', 'value': 'transport_time_days'},\n",
        "                    {'label': 'Inventory Coverage (days)', 'value': 'inventory_coverage_days'},\n",
        "                    {'label': 'Capacity Utilization (%)', 'value': 'capacity_utilization'},\n",
        "                    {'label': 'Demand Forecast', 'value': 'demand_forecast'},\n",
        "                    {'label': 'Order Quantity', 'value': 'order_quantity'}\n",
        "                ],\n",
        "                value='lead_time_days',\n",
        "                clearable=False\n",
        "            ),\n",
        "            dcc.Graph(id=\"time-series-chart\")\n",
        "        ], style={\"width\": \"49%\", \"display\": \"inline-block\", \"vertical-align\": \"top\"}),\n",
        "\n",
        "        # Right column - Anomaly distribution\n",
        "        html.Div([\n",
        "            html.H3(\"Anomaly Distribution by Category\", style={\"text-align\": \"center\"}),\n",
        "            dcc.Graph(id=\"category-anomaly-chart\")\n",
        "        ], style={\"width\": \"49%\", \"display\": \"inline-block\", \"vertical-align\": \"top\"})\n",
        "    ]),\n",
        "\n",
        "    # Second charts row\n",
        "    html.Div([\n",
        "        # Left column - Geographic distribution\n",
        "        html.Div([\n",
        "            html.H3(\"Regional Anomaly Distribution\", style={\"text-align\": \"center\"}),\n",
        "            dcc.Graph(id=\"regional-chart\")\n",
        "        ], style={\"width\": \"49%\", \"display\": \"inline-block\", \"vertical-align\": \"top\"}),\n",
        "\n",
        "        # Right column - Correlation matrix\n",
        "        html.Div([\n",
        "            html.H3(\"Feature Correlation Matrix\", style={\"text-align\": \"center\"}),\n",
        "            dcc.Graph(id=\"correlation-chart\")\n",
        "        ], style={\"width\": \"49%\", \"display\": \"inline-block\", \"vertical-align\": \"top\"})\n",
        "    ]),\n",
        "\n",
        "    # Anomaly table\n",
        "    html.Div([\n",
        "        html.H3(\"Detected Anomalies\", style={\"text-align\": \"center\"}),\n",
        "        dash_table.DataTable(\n",
        "            id='anomaly-table',\n",
        "            columns=[\n",
        "                {\"name\": \"Date\", \"id\": \"date\"},\n",
        "                {\"name\": \"Region\", \"id\": \"region\"},\n",
        "                {\"name\": \"Category\", \"id\": \"category\"},\n",
        "                {\"name\": \"Supplier\", \"id\": \"supplier\"},\n",
        "                {\"name\": \"Lead Time\", \"id\": \"lead_time_days\"},\n",
        "                {\"name\": \"Transport Time\", \"id\": \"transport_time_days\"},\n",
        "                {\"name\": \"Inventory Coverage\", \"id\": \"inventory_coverage_days\"},\n",
        "                {\"name\": \"Capacity Utilization\", \"id\": \"capacity_utilization\"},\n",
        "                {\"name\": \"Risk Score\", \"id\": \"risk_score\"}\n",
        "            ],\n",
        "            style_table={'overflowX': 'auto'},\n",
        "            style_cell={\n",
        "                'textAlign': 'left',\n",
        "                'padding': '5px',\n",
        "                'minWidth': '100px'\n",
        "            },\n",
        "            style_header={\n",
        "                'backgroundColor': '#2c3e50',\n",
        "                'color': 'white',\n",
        "                'fontWeight': 'bold'\n",
        "            },\n",
        "            style_data_conditional=[\n",
        "                {\n",
        "                    'if': {'column_id': 'risk_score', 'filter_query': '{risk_score} > 80'},\n",
        "                    'backgroundColor': '#e74c3c',\n",
        "                    'color': 'white'\n",
        "                },\n",
        "                {\n",
        "                    'if': {'column_id': 'risk_score', 'filter_query': '{risk_score} > 50 && {risk_score} <= 80'},\n",
        "                    'backgroundColor': '#f39c12',\n",
        "                    'color': 'white'\n",
        "                },\n",
        "                {\n",
        "                    'if': {'column_id': 'risk_score', 'filter_query': '{risk_score} <= 50'},\n",
        "                    'backgroundColor': '#27ae60',\n",
        "                    'color': 'white'\n",
        "                }\n",
        "            ],\n",
        "            page_size=10\n",
        "        )\n",
        "    ], style={\"margin\": \"20px 0px\"}),\n",
        "\n",
        "    # Footer\n",
        "    html.Div([\n",
        "        html.P(\"FMCG Supply Chain Anomaly Detection Dashboard | Hackathon Project\",\n",
        "               style={\"margin-bottom\": \"0px\", \"color\": \"white\"})\n",
        "    ], style={\"text-align\": \"center\", \"padding\": \"1rem\", \"background-color\": \"#2c3e50\", \"margin-top\": \"20px\"})\n",
        "], style={\"max-width\": \"1200px\", \"margin\": \"0 auto\", \"font-family\": \"Arial, sans-serif\"})\n",
        "\n",
        "# Add custom CSS\n",
        "app.index_string = '''\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "    <head>\n",
        "        {%metas%}\n",
        "        <title>{%title%}</title>\n",
        "        {%favicon%}\n",
        "        {%css%}\n",
        "        <style>\n",
        "            .kpi-card {\n",
        "                background-color: white;\n",
        "                border-radius: 5px;\n",
        "                box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n",
        "                padding: 15px;\n",
        "                width: 23%;\n",
        "            }\n",
        "\n",
        "            @media (max-width: 768px) {\n",
        "                .kpi-card {\n",
        "                    width: 48%;\n",
        "                    margin-bottom: 10px;\n",
        "                }\n",
        "            }\n",
        "        </style>\n",
        "    </head>\n",
        "    <body>\n",
        "        {%app_entry%}\n",
        "        <footer>\n",
        "            {%config%}\n",
        "            {%scripts%}\n",
        "            {%renderer%}\n",
        "        </footer>\n",
        "    </body>\n",
        "</html>\n",
        "'''\n",
        "\n",
        "\n",
        "# Define callback functions\n",
        "@app.callback(\n",
        "    [Output(\"total-records\", \"children\"),\n",
        "     Output(\"total-anomalies\", \"children\"),\n",
        "     Output(\"anomaly-rate\", \"children\"),\n",
        "     Output(\"most-affected-region\", \"children\")],\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\")]\n",
        ")\n",
        "def update_kpis(start_date, end_date, regions, categories, suppliers, anomaly_filter):\n",
        "    # Filter data based on inputs\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Calculate KPIs\n",
        "    total_records = len(filtered_df)\n",
        "    total_anomalies = filtered_df['anomaly_detected'].sum()\n",
        "    anomaly_rate = f\"{(total_anomalies / total_records * 100):.2f}%\" if total_records > 0 else \"0.00%\"\n",
        "\n",
        "    # Most affected region\n",
        "    if total_anomalies > 0:\n",
        "        most_affected_region = filtered_df[filtered_df['anomaly_detected'] == 1]['region'].mode()[0]\n",
        "    else:\n",
        "        most_affected_region = \"N/A\"\n",
        "\n",
        "    return total_records, total_anomalies, anomaly_rate, most_affected_region\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"time-series-chart\", \"figure\"),\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\"),\n",
        "     Input(\"metric-selector\", \"value\")]\n",
        ")\n",
        "def update_time_series(start_date, end_date, regions, categories, suppliers, anomaly_filter, metric):\n",
        "    # Filter data\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Create time series chart\n",
        "    fig = px.line(\n",
        "        filtered_df,\n",
        "        x=\"date\",\n",
        "        y=metric,\n",
        "        title=f\"{metric.replace('_', ' ').title()} Over Time\",\n",
        "        labels={\"date\": \"Date\", metric: metric.replace('_', ' ').title()}\n",
        "    )\n",
        "\n",
        "    # Highlight anomalies\n",
        "    if anomaly_filter == 'all':\n",
        "        anomalies_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "        fig.add_trace(\n",
        "            go.Scatter(\n",
        "                x=anomalies_df['date'],\n",
        "                y=anomalies_df[metric],\n",
        "                mode='markers',\n",
        "                marker=dict(color='red', size=8),\n",
        "                name='Anomalies'\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=metric.replace('_', ' ').title(),\n",
        "        hovermode=\"x unified\"\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"category-anomaly-chart\", \"figure\"),\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\")]\n",
        ")\n",
        "def update_category_anomaly_chart(start_date, end_date, regions, categories, suppliers, anomaly_filter):\n",
        "    # Filter data\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Group by category and count anomalies\n",
        "    category_anomalies = filtered_df.groupby('category')['anomaly_detected'].sum().reset_index()\n",
        "\n",
        "    # Create bar chart\n",
        "    fig = px.bar(\n",
        "        category_anomalies,\n",
        "        x=\"category\",\n",
        "        y=\"anomaly_detected\",\n",
        "        title=\"Anomalies by Category\",\n",
        "        labels={\"category\": \"Category\", \"anomaly_detected\": \"Number of Anomalies\"}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Category\",\n",
        "        yaxis_title=\"Number of Anomalies\"\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"regional-chart\", \"figure\"),\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\")]\n",
        ")\n",
        "def update_regional_chart(start_date, end_date, regions, categories, suppliers, anomaly_filter):\n",
        "    # Filter data\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Group by region and count anomalies\n",
        "    regional_anomalies = filtered_df.groupby('region')['anomaly_detected'].sum().reset_index()\n",
        "\n",
        "    # Create bar chart\n",
        "    fig = px.bar(\n",
        "        regional_anomalies,\n",
        "        x=\"region\",\n",
        "        y=\"anomaly_detected\",\n",
        "        title=\"Anomalies by Region\",\n",
        "        labels={\"region\": \"Region\", \"anomaly_detected\": \"Number of Anomalies\"}\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        xaxis_title=\"Region\",\n",
        "        yaxis_title=\"Number of Anomalies\"\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"correlation-chart\", \"figure\"),\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\")]\n",
        ")\n",
        "def update_correlation_chart(start_date, end_date, regions, categories, suppliers, anomaly_filter):\n",
        "    # Filter data\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Select numerical features for correlation\n",
        "    numerical_features = [\n",
        "        'order_quantity', 'lead_time_days', 'transport_time_days',\n",
        "        'inventory_level', 'demand_forecast', 'production_capacity',\n",
        "        'inventory_coverage_days', 'capacity_utilization'\n",
        "    ]\n",
        "\n",
        "    corr = filtered_df[numerical_features].corr()\n",
        "\n",
        "    # Create heatmap\n",
        "    fig = go.Figure(\n",
        "        data=go.Heatmap(\n",
        "            z=corr.values,\n",
        "            x=corr.columns,\n",
        "            y=corr.columns,\n",
        "            colorscale='Viridis',\n",
        "            zmin=-1,\n",
        "            zmax=1\n",
        "        )\n",
        "    )\n",
        "\n",
        "    fig.update_layout(\n",
        "        title=\"Feature Correlation Matrix\",\n",
        "        xaxis_title=\"Features\",\n",
        "        yaxis_title=\"Features\"\n",
        "    )\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "@app.callback(\n",
        "    Output(\"anomaly-table\", \"data\"),\n",
        "    [Input(\"date-range\", \"start_date\"),\n",
        "     Input(\"date-range\", \"end_date\"),\n",
        "     Input(\"region-filter\", \"value\"),\n",
        "     Input(\"category-filter\", \"value\"),\n",
        "     Input(\"supplier-filter\", \"value\"),\n",
        "     Input(\"anomaly-filter\", \"value\")]\n",
        ")\n",
        "def update_anomaly_table(start_date, end_date, regions, categories, suppliers, anomaly_filter):\n",
        "    # Filter data\n",
        "    filtered_df = df[\n",
        "        (df['date'] >= start_date) &\n",
        "        (df['date'] <= end_date)\n",
        "    ]\n",
        "\n",
        "    if regions:\n",
        "        filtered_df = filtered_df[filtered_df['region'].isin(regions)]\n",
        "    if categories:\n",
        "        filtered_df = filtered_df[filtered_df['category'].isin(categories)]\n",
        "    if suppliers:\n",
        "        filtered_df = filtered_df[filtered_df['supplier'].isin(suppliers)]\n",
        "\n",
        "    if anomaly_filter == 'anomalies':\n",
        "        filtered_df = filtered_df[filtered_df['anomaly_detected'] == 1]\n",
        "\n",
        "    # Add a risk score (for demo purposes)\n",
        "    filtered_df['risk_score'] = np.random.randint(0, 100, size=len(filtered_df))\n",
        "\n",
        "    # Prepare data for table\n",
        "    table_data = filtered_df[[\n",
        "        'date', 'region', 'category', 'supplier',\n",
        "        'lead_time_days', 'transport_time_days',\n",
        "        'inventory_coverage_days', 'capacity_utilization',\n",
        "        'risk_score'\n",
        "    ]].to_dict('records')\n",
        "\n",
        "    return table_data\n",
        "\n",
        "\n",
        "# Run the app\n",
        "if __name__ == \"__main__\":\n",
        "    app.server.run(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gr8CIMIC8w7M",
        "outputId": "675478d5-5bf6-4a96-afa3-0aa9dcc1db4b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z1XoUzhUAr71"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}